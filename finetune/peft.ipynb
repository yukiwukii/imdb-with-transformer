{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PrefixTuningConfig, PromptEncoderConfig, IA3Config, PeftModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=int(seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HamsterShiu/BERT_MLM\", \n",
    "    subfolder=\"checkpoint-95000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524b0f1d9fa487699441b9ff7b1da18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(lambda e: tokenizer(e[\"text\"], truncation=True, padding=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb = tokenized_imdb.remove_columns([\"text\"])\n",
    "tokenized_imdb = tokenized_imdb.rename_column(\"label\", \"labels\")\n",
    "tokenized_imdb.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_methods = {\n",
    "    \"LoRA\": LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"key\", \"value\"],\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    ),\n",
    "    \"Prefix Tuning\": PrefixTuningConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        num_virtual_tokens=20,\n",
    "        prefix_projection=True,\n",
    "        encoder_hidden_size=768,\n",
    "    ),\n",
    "    \"IA続\": IA3Config(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n",
    "        feedforward_modules=[\"output.dense\"],\n",
    "        inference_mode=False,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training with LoRA\n",
      "==================================================\n",
      "Finetuned model exists: ./SC4001/Assignment2/model/peft_bert_lora_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HamsterShiu/BERT_MLM and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4010346/3146117098.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training with Prefix Tuning\n",
      "==================================================\n",
      "Finetuned model exists: ./SC4001/Assignment2/model/peft_bert_prefix_tuning_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HamsterShiu/BERT_MLM and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4010346/3146117098.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training with IA続\n",
      "==================================================\n",
      "Finetuned model exists: ./SC4001/Assignment2/model/peft_bert_ia続_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HamsterShiu/BERT_MLM and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4010346/3146117098.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 02:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for method_name, peft_config in peft_methods.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {method_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    checkpoint_path = f\"./SC4001/Assignment2/model/peft_bert_{method_name.lower().replace(' ', '_')}_final\"\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Finetuned model exists: {checkpoint_path}\")\n",
    "        \n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"HamsterShiu/BERT_MLM\",\n",
    "            subfolder=\"checkpoint-95000\",\n",
    "            num_labels=2\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "        \n",
    "        # Use pre-calculated stats if available (you'd need to save this info separately)\n",
    "        if method_name in results:\n",
    "            print(f\"Using existing results for {method_name}\")\n",
    "        else:\n",
    "            # Evaluate the model\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./SC4001/Assignment2/model/peft_bert_{method_name.lower().replace(' ', '_')}_eval\",\n",
    "                per_device_eval_batch_size=16,\n",
    "                report_to=\"none\",\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                eval_dataset=tokenized_imdb[\"test\"],\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            \n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Count trainable parameters\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            # Add to results without training time (since it was pre-trained)\n",
    "            results[method_name] = {\n",
    "                \"trainable_params\": trainable_params,\n",
    "                \"trainable_params_percentage\": trainable_params/total_params*100,\n",
    "                \"eval_results\": eval_results\n",
    "            }\n",
    "            \n",
    "    else:\n",
    "        print(\"Finetuned model does not exist. Finetuning now.\")\n",
    "        \n",
    "        # Load a fresh base model for each method\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"HamsterShiu/BERT_MLM\",\n",
    "            subfolder=\"checkpoint-95000\",\n",
    "            num_labels=2\n",
    "        )\n",
    "        \n",
    "        # Apply the PEFT configuration\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params} ({trainable_params/total_params*100:.2f}%)\")\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./SC4001/Assignment2/model/peft_bert_{method_name.lower().replace(' ', '_')}\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=2,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False,\n",
    "            logging_steps=100,\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_imdb[\"train\"],\n",
    "            eval_dataset=tokenized_imdb[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Calculate training time\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Save results\n",
    "        results[method_name] = {\n",
    "            \"training_time\": training_time,\n",
    "            \"training_time_formatted\": format_time(training_time),\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"trainable_params_percentage\": trainable_params/total_params*100,\n",
    "            \"eval_results\": eval_results\n",
    "        }\n",
    "        \n",
    "        # Save PEFT model\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        print(f\"Saved model to {checkpoint_path}\")\n",
    "        \n",
    "        print(f\"\\nTraining time: {format_time(training_time)}\")\n",
    "        print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PEFT Methods Comparison\n",
      "================================================================================\n",
      "Method          Training Time   Params %   Accuracy   F1        \n",
      "--------------------------------------------------------------------------------\n",
      "LoRA            Pre-trained     0.00%      0.5743      0.3021\n",
      "Prefix Tuning   Pre-trained     0.00%      0.9011      0.9017\n",
      "IA続             Pre-trained     0.00%      0.9056      0.9062\n"
     ]
    }
   ],
   "source": [
    "# Print comparison of all methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PEFT Methods Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<15} {'Training Time':<15} {'Params %':<10} {'Accuracy':<10} {'F1':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for method, data in results.items():\n",
    "    # For pre-trained models that don't have training time recorded\n",
    "    training_time = data.get(\"training_time_formatted\", \"Pre-trained\")\n",
    "    \n",
    "    print(f\"{method:<15} {training_time:<15} {data['trainable_params_percentage']:.2f}%{' ':<5} {data['eval_results']['eval_accuracy']:.4f}{' ':<5} {data['eval_results']['eval_f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl",
   "language": "python",
   "name": "nndl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
